{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train_answer_file: 訓練Annotation File位置\n",
    "- train_txt_dir: 訓練資料位置\n",
    "- val_answer_file: 驗證Annotation File位置\n",
    "- val_txt_dir: 驗證資料位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_answer_file = \"data/train_answer.txt\"\n",
    "train_txt_dir = \"data/train_file\"\n",
    "\n",
    "val_answer_file = \"data/val_answer.txt\"\n",
    "val_txt_dir = \"data/val_file\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_annotation_file(lines):\n",
    "    entity_dict = {}\n",
    "    for line in lines:\n",
    "        items = line.strip('\\n').split('\\t')\n",
    "        if len(items) == 5:\n",
    "            item_list = (int(items[2]),int(items[3]),items[1])\n",
    "        elif len(items) == 6:\n",
    "            item_list = (int(items[2]),int(items[3]),items[1])\n",
    "\n",
    "        file_name = f\"{items[0]}.txt\"\n",
    "        if file_name not in entity_dict:\n",
    "            entity_dict[file_name] = [item_list]\n",
    "        else:\n",
    "            entity_dict[file_name].append(item_list)\n",
    "    return entity_dict\n",
    "\n",
    "def read_file(path):\n",
    "    with open(path , 'r' , encoding = 'utf-8-sig') as fr:\n",
    "        return fr.readlines()\n",
    "\n",
    "train_entity_dict = process_annotation_file(read_file(train_answer_file))\n",
    "val_entity_dict = process_annotation_file(read_file(val_answer_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def ann_gen(entity_dict, txt_dir):\n",
    "    ann_list = []\n",
    "    for k, v in entity_dict.items():\n",
    "        ann = []\n",
    "        # print(k)\n",
    "        file_path = os.path.join(txt_dir,k)\n",
    "        text_file = open(file_path, \"r\").read()\n",
    "        ann.append(text_file)\n",
    "        ann.append(v)\n",
    "        ann_list.append(ann)\n",
    "    return ann_list\n",
    "train_json = ann_gen(train_entity_dict, train_txt_dir)\n",
    "val_json = ann_gen(val_entity_dict, val_txt_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "輸出成json檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_object = json.dumps(train_json,indent=4)\n",
    "with open(\"train.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)\n",
    "\n",
    "json_object = json.dumps(val_json,indent=4)\n",
    "with open(\"val.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下載spacy的model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# !python -m spacy download en_core_web_trf\n",
    "# nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "!python -m spacy download en_core_web_lg\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文字轉成token，過濾掉不用的span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "def create_data(_data, out_name):\n",
    "    doc_bin = DocBin()\n",
    "    skip_num = 0\n",
    "    for training_example in tqdm(_data):\n",
    "        text = training_example[0]\n",
    "        labels = training_example[1]\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label in labels:\n",
    "            # span = doc.char_span(start, end, label=label)\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span is None:\n",
    "                skip_num+=1\n",
    "                # print(start, end, label)\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        filtered_ents = filter_spans(ents)\n",
    "        doc.ents = filtered_ents\n",
    "        doc_bin.add(doc)\n",
    "    doc_bin.to_disk(f\"{out_name}.spacy\")\n",
    "    print(skip_num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1734\n",
      "560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1734/1734 [00:14<00:00, 120.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 560/560 [00:04<00:00, 117.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463\n"
     ]
    }
   ],
   "source": [
    "train_data = json.load(open('train.json','r'))\n",
    "\n",
    "val_data = json.load(open('val.json','r'))\n",
    "\n",
    "create_data(train_data, \"train\")\n",
    "create_data(val_data, \"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "產生config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy init fill-config ./base_config.cfg ./config.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "開始training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: trf\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2023-12-07 09:17:43,306] [INFO] Set up nlp object from config\n",
      "[2023-12-07 09:17:43,314] [INFO] Pipeline: ['transformer', 'ner']\n",
      "[2023-12-07 09:17:43,317] [INFO] Created vocabulary\n",
      "[2023-12-07 09:17:43,317] [INFO] Finished initializing nlp object\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[2023-12-07 09:18:13,829] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  ------  ------  ------  ------\n",
      "  0       0       11816.88   1171.27    0.00    0.00    0.00    0.00\n",
      "  0     200      389294.02  50902.67   48.26   62.42   39.34    0.48\n",
      "  0     400       33308.83  16059.17   90.26   89.61   90.92    0.90\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train config.cfg --output ./ --paths.train ./train.spacy --paths.dev ./val.spacy --gpu-id 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
